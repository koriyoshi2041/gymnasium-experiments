#!/usr/bin/env python3
"""
REINFORCE (Policy Gradient) ç®—æ³•å®ç° - CartPole ç¯å¢ƒ

REINFORCE æ˜¯æœ€åŸºç¡€çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼š
1. ç”¨ç¥ç»ç½‘ç»œç›´æ¥è¾“å‡ºåŠ¨ä½œæ¦‚ç‡ï¼ˆç­–ç•¥ Ï€(a|s)ï¼‰
2. é‡‡æ ·å®Œæ•´è½¨è¿¹åï¼Œç”¨è’™ç‰¹å¡æ´›å›æŠ¥æ›´æ–°ç­–ç•¥
3. ç›®æ ‡ï¼šæœ€å¤§åŒ–æœŸæœ›ç´¯ç§¯å›æŠ¥ E[Î£Î³^t * r_t]

æ ¸å¿ƒå…¬å¼ï¼š
    âˆ‡J(Î¸) â‰ˆ Î£_t [âˆ‡log Ï€(a_t|s_t; Î¸) * G_t]
    å…¶ä¸­ G_t = Î£_{k=t}^T Î³^(k-t) * r_k æ˜¯ä» t æ—¶åˆ»å¼€å§‹çš„æŠ˜æ‰£å›æŠ¥
"""

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import matplotlib.pyplot as plt
import imageio
from collections import deque
import warnings
warnings.filterwarnings('ignore')


# ============================================================
# ç­–ç•¥ç½‘ç»œ (Policy Network)
# ============================================================
class PolicyNetwork(nn.Module):
    """
    ç­–ç•¥ç½‘ç»œï¼šè¾“å…¥çŠ¶æ€ sï¼Œè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ Ï€(Â·|s)
    
    CartPole çŠ¶æ€ç©ºé—´ï¼š4ç»´ï¼ˆä½ç½®ã€é€Ÿåº¦ã€è§’åº¦ã€è§’é€Ÿåº¦ï¼‰
    CartPole åŠ¨ä½œç©ºé—´ï¼š2ä¸ªç¦»æ•£åŠ¨ä½œï¼ˆå·¦æ¨ã€å³æ¨ï¼‰
    
    ç½‘ç»œç»“æ„ï¼šç®€å•çš„ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ
    """
    def __init__(self, state_dim: int = 4, hidden_dim: int = 128, action_dim: int = 2):
        super(PolicyNetwork, self).__init__()
        
        # ä¸¤å±‚å…¨è¿æ¥ + ReLU æ¿€æ´»
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),   # è¾“å…¥å±‚ -> éšè—å±‚
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),  # éšè—å±‚ -> éšè—å±‚
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),  # éšè—å±‚ -> è¾“å‡ºå±‚
            nn.Softmax(dim=-1)                  # Softmax è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
        )
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ï¼šçŠ¶æ€ -> åŠ¨ä½œæ¦‚ç‡"""
        return self.network(state)
    
    def act(self, state: np.ndarray, deterministic: bool = False) -> int:
        """
        æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: ç¯å¢ƒçŠ¶æ€ï¼ˆnumpyæ•°ç»„ï¼‰
            deterministic: æ˜¯å¦ç¡®å®šæ€§é€‰æ‹©ï¼ˆé€‰æ¦‚ç‡æœ€å¤§çš„åŠ¨ä½œï¼‰
        
        Returns:
            é€‰æ‹©çš„åŠ¨ä½œ
        """
        # è½¬æ¢ä¸º tensor
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        
        # è·å–åŠ¨ä½œæ¦‚ç‡
        with torch.no_grad():
            action_probs = self.forward(state_tensor)
        
        if deterministic:
            # ç¡®å®šæ€§ï¼šé€‰æ¦‚ç‡æœ€å¤§çš„
            action = torch.argmax(action_probs, dim=1).item()
        else:
            # éšæœºæ€§ï¼šæŒ‰æ¦‚ç‡é‡‡æ ·
            dist = Categorical(action_probs)
            action = dist.sample().item()
        
        return action


# ============================================================
# REINFORCE Agent
# ============================================================
class REINFORCEAgent:
    """
    REINFORCE ç®—æ³•å®ç°
    
    ç®—æ³•æµç¨‹ï¼š
    1. æ”¶é›†ä¸€æ•´æ¡è½¨è¿¹ (s_0, a_0, r_1, s_1, a_1, r_2, ...)
    2. è®¡ç®—æ¯ä¸ªæ—¶åˆ»çš„æŠ˜æ‰£å›æŠ¥ G_t
    3. è®¡ç®—ç­–ç•¥æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
    
    æ”¹è¿›ï¼šä½¿ç”¨ baselineï¼ˆå›æŠ¥æ ‡å‡†åŒ–ï¼‰å‡å°‘æ–¹å·®
    """
    def __init__(self, state_dim: int = 4, action_dim: int = 2, 
                 hidden_dim: int = 128, learning_rate: float = 1e-3, 
                 gamma: float = 0.99):
        """
        Args:
            state_dim: çŠ¶æ€ç»´åº¦
            action_dim: åŠ¨ä½œæ•°é‡
            hidden_dim: éšè—å±‚å¤§å°
            learning_rate: å­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
        """
        self.gamma = gamma
        
        # åˆå§‹åŒ–ç­–ç•¥ç½‘ç»œ
        self.policy = PolicyNetwork(state_dim, hidden_dim, action_dim)
        
        # Adam ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)
        
        # å­˜å‚¨è½¨è¿¹æ•°æ®
        self.saved_log_probs = []  # åŠ¨ä½œçš„ log æ¦‚ç‡
        self.rewards = []           # è·å¾—çš„å¥–åŠ±
    
    def select_action(self, state: np.ndarray) -> int:
        """
        é€‰æ‹©åŠ¨ä½œå¹¶ä¿å­˜ log æ¦‚ç‡ï¼ˆç”¨äºåç»­æ¢¯åº¦è®¡ç®—ï¼‰
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        action_probs = self.policy(state_tensor)
        
        # åˆ›å»ºåˆ†ç±»åˆ†å¸ƒ
        dist = Categorical(action_probs)
        
        # é‡‡æ ·åŠ¨ä½œ
        action = dist.sample()
        
        # ä¿å­˜ log æ¦‚ç‡ï¼ˆæ¢¯åº¦ä¼šé€šè¿‡è¿™é‡Œåå‘ä¼ æ’­ï¼‰
        self.saved_log_probs.append(dist.log_prob(action))
        
        return action.item()
    
    def store_reward(self, reward: float):
        """å­˜å‚¨å¥–åŠ±"""
        self.rewards.append(reward)
    
    def compute_returns(self) -> torch.Tensor:
        """
        è®¡ç®—æŠ˜æ‰£å›æŠ¥ G_t = Î£_{k=t}^T Î³^(k-t) * r_k
        
        ä»åå¾€å‰è®¡ç®—ï¼Œæ•ˆç‡æ›´é«˜ï¼š
        G_T = r_T
        G_{T-1} = r_{T-1} + Î³ * G_T
        G_{T-2} = r_{T-2} + Î³ * G_{T-1}
        ...
        """
        returns = []
        G = 0
        
        # ä»æœ€åä¸€ä¸ªæ—¶åˆ»å¾€å‰ç®—
        for r in reversed(self.rewards):
            G = r + self.gamma * G
            returns.insert(0, G)
        
        returns = torch.tensor(returns, dtype=torch.float32)
        
        # æ ‡å‡†åŒ–ï¼ˆå‡å°‘æ–¹å·®ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„ baselineï¼‰
        if len(returns) > 1:
            returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        return returns
    
    def update(self) -> float:
        """
        æ›´æ–°ç­–ç•¥ç½‘ç»œ
        
        ç­–ç•¥æ¢¯åº¦ï¼šâˆ‡J(Î¸) â‰ˆ Î£_t [âˆ‡log Ï€(a_t|s_t; Î¸) * G_t]
        
        PyTorch ä¸­ï¼š
        - æˆ‘ä»¬è¦æœ€å¤§åŒ– J(Î¸)ï¼Œç­‰ä»·äºæœ€å°åŒ– -J(Î¸)
        - loss = -Î£_t [log Ï€(a_t|s_t) * G_t]
        """
        # è®¡ç®—æŠ˜æ‰£å›æŠ¥
        returns = self.compute_returns()
        
        # è®¡ç®— policy loss
        policy_loss = []
        for log_prob, G in zip(self.saved_log_probs, returns):
            # è´Ÿå·ï¼šå› ä¸ºæˆ‘ä»¬è¦æœ€å¤§åŒ–æœŸæœ›å›æŠ¥ï¼Œä½†ä¼˜åŒ–å™¨åšçš„æ˜¯æœ€å°åŒ–
            policy_loss.append(-log_prob * G)
        
        # åˆå¹¶æ‰€æœ‰æ—¶åˆ»çš„ loss
        loss = torch.stack(policy_loss).sum()
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # æ¸…ç©ºè½¨è¿¹ç¼“å­˜
        loss_value = loss.item()
        self.saved_log_probs = []
        self.rewards = []
        
        return loss_value


# ============================================================
# è®­ç»ƒå‡½æ•°
# ============================================================
def train(learning_rate: float = 1e-3, 
          num_episodes: int = 2000,
          target_reward: float = 495.0,
          patience: int = 100,
          verbose: bool = True) -> tuple:
    """
    è®­ç»ƒ REINFORCE agent
    
    Args:
        learning_rate: å­¦ä¹ ç‡
        num_episodes: æœ€å¤§è®­ç»ƒå›åˆæ•°
        target_reward: ç›®æ ‡åˆ†æ•°ï¼ˆCartPole æ»¡åˆ† 500ï¼‰
        patience: è¿ç»­è¾¾åˆ°ç›®æ ‡åˆ†æ•°çš„æ¬¡æ•°æ‰ç®—"ç¨³å®š"
        verbose: æ˜¯å¦æ‰“å°è®­ç»ƒè¿‡ç¨‹
    
    Returns:
        (agent, rewards_history)
    """
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make("CartPole-v1")
    
    # åˆ›å»º agent
    agent = REINFORCEAgent(
        state_dim=env.observation_space.shape[0],
        action_dim=env.action_space.n,
        learning_rate=learning_rate
    )
    
    # è®°å½•è®­ç»ƒå†å²
    rewards_history = []
    recent_rewards = deque(maxlen=patience)
    
    for episode in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0
        
        # æ”¶é›†ä¸€æ•´æ¡è½¨è¿¹
        while True:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # å­˜å‚¨å¥–åŠ±
            agent.store_reward(reward)
            episode_reward += reward
            
            state = next_state
            
            if done:
                break
        
        # æ›´æ–°ç­–ç•¥
        agent.update()
        
        # è®°å½•
        rewards_history.append(episode_reward)
        recent_rewards.append(episode_reward)
        avg_reward = np.mean(recent_rewards)
        
        # æ‰“å°è¿›åº¦
        if verbose and (episode + 1) % 100 == 0:
            print(f"[lr={learning_rate}] Episode {episode+1}, "
                  f"Avg Reward (last {patience}): {avg_reward:.1f}")
        
        # æ£€æŸ¥æ˜¯å¦ç¨³å®šè¾¾åˆ°ç›®æ ‡
        if len(recent_rewards) >= patience and avg_reward >= target_reward:
            if verbose:
                print(f"[lr={learning_rate}] ğŸ‰ Solved in {episode+1} episodes! "
                      f"Avg: {avg_reward:.1f}")
            break
    
    env.close()
    return agent, rewards_history


# ============================================================
# è¯„ä¼°å’Œå½•åˆ¶ GIF
# ============================================================
def record_gif(policy: PolicyNetwork, filename: str = "cartpole_reinforce.gif"):
    """
    ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥å½•åˆ¶ GIF
    """
    env = gym.make("CartPole-v1", render_mode="rgb_array")
    frames = []
    
    obs, _ = env.reset()
    
    for _ in range(500):
        frames.append(env.render())
        action = policy.act(obs, deterministic=True)  # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
        obs, r, done, trunc, _ = env.step(action)
        if done or trunc:
            break
    
    env.close()
    
    # ä¿å­˜ GIF
    imageio.mimsave(filename, frames, fps=30)
    print(f"âœ… GIF saved: {filename} ({len(frames)} frames)")
    
    return len(frames)


# ============================================================
# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
# ============================================================
def plot_training_curves(results: dict, filename: str = "training_curve.png"):
    """
    ç»˜åˆ¶ä¸åŒå­¦ä¹ ç‡çš„è®­ç»ƒæ›²çº¿å¯¹æ¯”å›¾
    
    Args:
        results: {learning_rate: rewards_history}
    """
    plt.figure(figsize=(12, 6))
    
    colors = ['#2ecc71', '#3498db', '#e74c3c']  # ç»¿ã€è“ã€çº¢
    
    for i, (lr, rewards) in enumerate(results.items()):
        episodes = range(1, len(rewards) + 1)
        
        # åŸå§‹æ›²çº¿ï¼ˆé€æ˜åº¦ä½ï¼‰
        plt.plot(episodes, rewards, alpha=0.2, color=colors[i])
        
        # å¹³æ»‘æ›²çº¿ï¼ˆç§»åŠ¨å¹³å‡ï¼‰
        window = 50
        if len(rewards) >= window:
            smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')
            plt.plot(range(window, len(rewards)+1), smoothed, 
                    label=f'lr={lr}', color=colors[i], linewidth=2)
        else:
            plt.plot(episodes, rewards, label=f'lr={lr}', color=colors[i])
    
    plt.axhline(y=500, color='gold', linestyle='--', label='Target (500)', linewidth=2)
    plt.xlabel('Episode', fontsize=12)
    plt.ylabel('Reward', fontsize=12)
    plt.title('REINFORCE on CartPole-v1: Learning Rate Comparison', fontsize=14)
    plt.legend(loc='lower right', fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    plt.savefig(filename, dpi=150)
    plt.close()
    print(f"âœ… Training curve saved: {filename}")


# ============================================================
# ä¸»å‡½æ•°
# ============================================================
def main():
    print("=" * 60)
    print("REINFORCE (Policy Gradient) - CartPole Experiment")
    print("=" * 60)
    
    # è¦å¯¹æ¯”çš„å­¦ä¹ ç‡
    learning_rates = [1e-2, 3e-3, 1e-3]
    
    # å­˜å‚¨ç»“æœ
    results = {}
    best_agent = None
    best_lr = None
    best_episodes = float('inf')
    
    # è®­ç»ƒä¸åŒå­¦ä¹ ç‡
    for lr in learning_rates:
        print(f"\n{'='*40}")
        print(f"Training with learning rate = {lr}")
        print(f"{'='*40}")
        
        agent, rewards = train(
            learning_rate=lr,
            num_episodes=2000,
            target_reward=495.0,
            patience=100,
            verbose=True
        )
        
        results[lr] = rewards
        
        # è®°å½•æœ€å¥½çš„æ¨¡å‹ï¼ˆæœ€å¿«è¾¾åˆ°ç›®æ ‡çš„ï¼‰
        if len(rewards) < best_episodes:
            best_episodes = len(rewards)
            best_agent = agent
            best_lr = lr
    
    print(f"\n{'='*60}")
    print(f"ğŸ† Best learning rate: {best_lr} (solved in {best_episodes} episodes)")
    print(f"{'='*60}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¯¹æ¯”å›¾
    plot_training_curves(results, "training_curve.png")
    
    # ä¿å­˜æœ€å¥½çš„æ¨¡å‹
    torch.save(best_agent.policy.state_dict(), "policy_model.pth")
    print(f"âœ… Best model saved: policy_model.pth")
    
    # ç”¨æœ€å¥½çš„æ¨¡å‹å½•åˆ¶ GIF
    print(f"\nğŸ“¹ Recording GIF with best model (lr={best_lr})...")
    record_gif(best_agent.policy, "cartpole_reinforce.gif")
    
    print("\n" + "=" * 60)
    print("âœ… All outputs generated:")
    print("   - reinforce.py")
    print("   - policy_model.pth")
    print("   - training_curve.png")
    print("   - cartpole_reinforce.gif")
    print("=" * 60)


if __name__ == "__main__":
    main()
