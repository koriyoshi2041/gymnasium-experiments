#!/usr/bin/env python3
"""
BipedalWalker-v3 PPO Training Script
ä½¿ç”¨ Stable-Baselines3 è®­ç»ƒåŒè¶³æœºå™¨äººèµ°è·¯

ä½œè€…: Rios
æ—¥æœŸ: 2025-07
"""

import os
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import imageio
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.env_util import make_vec_env
import torch

# è®¾ç½®éšæœºç§å­
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)

# è®­ç»ƒå‚æ•°
TOTAL_TIMESTEPS = 1_000_000  # 1M steps
EVAL_FREQ = 10_000
N_EVAL_EPISODES = 5
MODEL_SAVE_PATH = "bipedal_ppo_model"
LOG_DIR = "./logs/"

os.makedirs(LOG_DIR, exist_ok=True)


class TrainingCallback(BaseCallback):
    """è‡ªå®šä¹‰å›è°ƒï¼šè®°å½•è®­ç»ƒæ›²çº¿"""
    
    def __init__(self, check_freq=1000, verbose=1):
        super().__init__(verbose)
        self.check_freq = check_freq
        self.rewards = []
        self.timesteps = []
        self.episode_rewards = []
        self.current_episode_reward = 0
        
    def _on_step(self) -> bool:
        # ç´¯ç§¯å½“å‰ episode å¥–åŠ±
        if len(self.locals.get('rewards', [])) > 0:
            self.current_episode_reward += self.locals['rewards'][0]
            
        # æ£€æŸ¥ episode æ˜¯å¦ç»“æŸ
        if len(self.locals.get('dones', [])) > 0 and self.locals['dones'][0]:
            self.episode_rewards.append(self.current_episode_reward)
            self.current_episode_reward = 0
            
            # å®šæœŸè®°å½•
            if len(self.episode_rewards) % 10 == 0:
                avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)
                self.rewards.append(avg_reward)
                self.timesteps.append(self.num_timesteps)
                
                if self.verbose:
                    print(f"Timesteps: {self.num_timesteps:,} | Episodes: {len(self.episode_rewards)} | Avg Reward (last 100): {avg_reward:.2f}")
        
        return True
    
    def get_training_data(self):
        return self.timesteps, self.rewards


def create_env():
    """åˆ›å»ºè®­ç»ƒç¯å¢ƒ"""
    env = gym.make("BipedalWalker-v3")
    env = Monitor(env, LOG_DIR)
    return env


def train():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=" * 60)
    print("ğŸ¦¿ BipedalWalker-v3 PPO Training")
    print("=" * 60)
    print(f"Total timesteps: {TOTAL_TIMESTEPS:,}")
    print(f"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}")
    print("=" * 60)
    
    # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
    env = make_vec_env("BipedalWalker-v3", n_envs=4, seed=SEED)
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    eval_env = make_vec_env("BipedalWalker-v3", n_envs=1, seed=SEED + 100)
    
    # PPO è¶…å‚æ•°ï¼ˆé’ˆå¯¹ BipedalWalker ä¼˜åŒ–ï¼‰
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.0,
        vf_coef=0.5,
        max_grad_norm=0.5,
        verbose=1,
        tensorboard_log=None,  # ç¦ç”¨ tensorboard
        seed=SEED,
    )
    
    # å›è°ƒ
    training_callback = TrainingCallback(check_freq=1000, verbose=1)
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=LOG_DIR,
        log_path=LOG_DIR,
        eval_freq=EVAL_FREQ,
        n_eval_episodes=N_EVAL_EPISODES,
        deterministic=True,
        render=False,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸƒ Starting training...")
    model.learn(
        total_timesteps=TOTAL_TIMESTEPS,
        callback=[training_callback, eval_callback],
        progress_bar=True,
    )
    
    # ä¿å­˜æ¨¡å‹
    model.save(MODEL_SAVE_PATH)
    print(f"\nâœ… Model saved to {MODEL_SAVE_PATH}.zip")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curve(training_callback)
    
    # å½•åˆ¶ GIF
    record_gif(model)
    
    env.close()
    eval_env.close()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ Training complete!")
    print("=" * 60)


def plot_training_curve(callback):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    timesteps, rewards = callback.get_training_data()
    
    if len(timesteps) < 2:
        print("âš ï¸ Not enough data to plot training curve")
        return
    
    plt.figure(figsize=(12, 6))
    
    # åŸå§‹æ•°æ®
    plt.subplot(1, 2, 1)
    plt.plot(timesteps, rewards, 'b-', alpha=0.7, label='Avg Reward (100 eps)')
    plt.xlabel('Timesteps')
    plt.ylabel('Average Reward')
    plt.title('BipedalWalker-v3 PPO Training Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å¹³æ»‘æ›²çº¿
    plt.subplot(1, 2, 2)
    if len(rewards) > 10:
        smoothed = np.convolve(rewards, np.ones(10)/10, mode='valid')
        plt.plot(timesteps[:len(smoothed)], smoothed, 'r-', linewidth=2, label='Smoothed (10-pt avg)')
    plt.plot(timesteps, rewards, 'b-', alpha=0.3, label='Raw')
    plt.xlabel('Timesteps')
    plt.ylabel('Average Reward')
    plt.title('Smoothed Training Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('training_curve.png', dpi=150)
    print("ğŸ“Š Training curve saved to training_curve.png")
    plt.close()


def record_gif(model, filename="bipedal_walker.gif", n_frames=500):
    """å½•åˆ¶ GIF åŠ¨ç”»"""
    print(f"\nğŸ¬ Recording GIF ({n_frames} frames)...")
    
    env = gym.make("BipedalWalker-v3", render_mode="rgb_array")
    frames = []
    
    obs, _ = env.reset(seed=SEED)
    total_reward = 0
    episode_count = 0
    
    for i in range(n_frames):
        frame = env.render()
        frames.append(frame)
        
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, _ = env.step(action)
        total_reward += reward
        
        if terminated or truncated:
            episode_count += 1
            print(f"  Episode {episode_count} reward: {total_reward:.2f}")
            total_reward = 0
            obs, _ = env.reset()
    
    env.close()
    
    # ä¿å­˜ GIF
    imageio.mimsave(filename, frames, fps=30, loop=0)
    print(f"âœ… GIF saved to {filename}")
    print(f"   - Frames: {len(frames)}")
    print(f"   - Duration: {len(frames)/30:.1f}s")


def evaluate_model(model_path=MODEL_SAVE_PATH):
    """è¯„ä¼°å·²è®­ç»ƒçš„æ¨¡å‹"""
    print("\nğŸ“Š Evaluating model...")
    
    model = PPO.load(model_path)
    env = gym.make("BipedalWalker-v3")
    
    rewards = []
    for ep in range(10):
        obs, _ = env.reset()
        total_reward = 0
        done = False
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, _ = env.step(action)
            total_reward += reward
            done = terminated or truncated
        
        rewards.append(total_reward)
        print(f"  Episode {ep+1}: {total_reward:.2f}")
    
    env.close()
    
    print(f"\nğŸ“ˆ Results over 10 episodes:")
    print(f"   Mean: {np.mean(rewards):.2f}")
    print(f"   Std:  {np.std(rewards):.2f}")
    print(f"   Min:  {np.min(rewards):.2f}")
    print(f"   Max:  {np.max(rewards):.2f}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="BipedalWalker PPO Training")
    parser.add_argument("--train", action="store_true", help="Train the model")
    parser.add_argument("--eval", action="store_true", help="Evaluate the model")
    parser.add_argument("--gif", action="store_true", help="Record GIF only")
    parser.add_argument("--timesteps", type=int, default=TOTAL_TIMESTEPS, help="Total training timesteps")
    
    args = parser.parse_args()
    
    if args.timesteps != TOTAL_TIMESTEPS:
        TOTAL_TIMESTEPS = args.timesteps
    
    if args.gif:
        model = PPO.load(MODEL_SAVE_PATH)
        record_gif(model)
    elif args.eval:
        evaluate_model()
    else:
        train()
