"""
LunarLander-v3 DQN è®­ç»ƒè„šæœ¬
ç›®æ ‡: å¹³å‡ reward > 200
"""
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
from collections import deque
import imageio
import time
import os

from dqn import DQNAgent, device

# é…ç½®
MAX_EPISODES = 1000
TARGET_REWARD = 200
WINDOW_SIZE = 100  # è®¡ç®—å¹³å‡å¥–åŠ±çš„çª—å£
SAVE_INTERVAL = 100

def train():
    print(f"Using device: {device}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make("LunarLander-v3")
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    
    print(f"State dim: {state_dim}, Action dim: {action_dim}")
    
    # åˆ›å»º Agent
    agent = DQNAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        lr=5e-4,
        gamma=0.99,
        epsilon_start=1.0,
        epsilon_end=0.01,
        epsilon_decay=0.995,
        batch_size=64,
        target_update=10
    )
    
    # è®°å½•
    rewards_history = []
    avg_rewards_history = []
    recent_rewards = deque(maxlen=WINDOW_SIZE)
    best_avg_reward = -float('inf')
    
    start_time = time.time()
    
    for episode in range(1, MAX_EPISODES + 1):
        state, _ = env.reset()
        episode_reward = 0
        done = False
        
        while not done:
            action = agent.select_action(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            agent.store_transition(state, action, reward, next_state, done)
            agent.learn()
            
            state = next_state
            episode_reward += reward
        
        agent.decay_epsilon()
        
        rewards_history.append(episode_reward)
        recent_rewards.append(episode_reward)
        avg_reward = np.mean(recent_rewards)
        avg_rewards_history.append(avg_reward)
        
        # æ‰“å°è¿›åº¦
        if episode % 10 == 0:
            elapsed = time.time() - start_time
            print(f"Episode {episode:4d} | "
                  f"Reward: {episode_reward:7.1f} | "
                  f"Avg(100): {avg_reward:7.1f} | "
                  f"Epsilon: {agent.epsilon:.3f} | "
                  f"Time: {elapsed:.0f}s")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_reward > best_avg_reward and len(recent_rewards) == WINDOW_SIZE:
            best_avg_reward = avg_reward
            agent.save("model_best.pth")
        
        # å®šæœŸä¿å­˜
        if episode % SAVE_INTERVAL == 0:
            agent.save(f"model_ep{episode}.pth")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾æ ‡
        if avg_reward >= TARGET_REWARD and len(recent_rewards) == WINDOW_SIZE:
            print(f"\nğŸ‰ Solved in {episode} episodes! Avg reward: {avg_reward:.1f}")
            agent.save("model.pth")
            break
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    agent.save("model.pth")
    env.close()
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curve(rewards_history, avg_rewards_history)
    
    return agent, rewards_history

def plot_training_curve(rewards, avg_rewards):
    """ç»˜åˆ¶å¹¶ä¿å­˜è®­ç»ƒæ›²çº¿"""
    plt.figure(figsize=(12, 5))
    
    # å¥–åŠ±æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(rewards, alpha=0.6, label='Episode Reward')
    plt.plot(avg_rewards, color='red', linewidth=2, label=f'Avg (window={WINDOW_SIZE})')
    plt.axhline(y=TARGET_REWARD, color='green', linestyle='--', label=f'Target ({TARGET_REWARD})')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title('LunarLander-v3 DQN Training')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æœ€å100è½®çš„åˆ†å¸ƒ
    plt.subplot(1, 2, 2)
    last_100 = rewards[-100:] if len(rewards) >= 100 else rewards
    plt.hist(last_100, bins=20, edgecolor='black', alpha=0.7)
    plt.axvline(x=np.mean(last_100), color='red', linestyle='--', 
                label=f'Mean: {np.mean(last_100):.1f}')
    plt.xlabel('Reward')
    plt.ylabel('Count')
    plt.title('Last 100 Episodes Distribution')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('training_curve.png', dpi=150)
    plt.close()
    print("âœ… Saved training_curve.png")

def record_gif(model_path="model.pth", output_path="lunar_lander_trained.gif", episodes=3):
    """å½•åˆ¶è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"\nğŸ“¹ Recording GIF with {model_path}...")
    
    env = gym.make("LunarLander-v3", render_mode="rgb_array")
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    
    agent = DQNAgent(state_dim=state_dim, action_dim=action_dim)
    agent.load(model_path)
    
    frames = []
    total_rewards = []
    
    for ep in range(episodes):
        obs, _ = env.reset()
        episode_reward = 0
        
        while True:
            frames.append(env.render())
            action = agent.select_action(obs, greedy=True)
            obs, reward, done, trunc, _ = env.step(action)
            episode_reward += reward
            
            if done or trunc:
                # æ·»åŠ å‡ å¸§ç»“æŸç”»é¢
                for _ in range(15):
                    frames.append(env.render())
                break
        
        total_rewards.append(episode_reward)
        print(f"  Episode {ep+1}: Reward = {episode_reward:.1f}")
    
    env.close()
    
    # ä¿å­˜ GIF
    imageio.mimsave(output_path, frames, fps=30)
    print(f"âœ… Saved {output_path} ({len(frames)} frames)")
    print(f"   Average reward: {np.mean(total_rewards):.1f}")
    
    return total_rewards

if __name__ == "__main__":
    # åˆ‡æ¢åˆ°è„šæœ¬ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(script_dir)
    
    print("=" * 60)
    print("  LunarLander-v3 DQN Training")
    print("=" * 60)
    
    # è®­ç»ƒ
    agent, rewards = train()
    
    # å½•åˆ¶ GIF
    record_gif()
    
    print("\n" + "=" * 60)
    print("  Training Complete!")
    print("=" * 60)
